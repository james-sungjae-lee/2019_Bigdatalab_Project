{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver import ChromeOptions\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list_csv(mylist, tag_name):\n",
    "    file_name = tag_name + '_tag_link.csv'\n",
    "    with open(file_name, \"w\") as file:\n",
    "        for line in mylist:\n",
    "            file.write(line + ',\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling_links(my_tag, num_of_crawling_pages):\n",
    "    options = ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--disable-extensions')\n",
    "    options.add_argument('--no-sandbox')\n",
    "\n",
    "    driver = Chrome(options = options)\n",
    "    driver.implicitly_wait(1)\n",
    "\n",
    "    base_url = \"https://www.instagram.com/explore/tags/\"\n",
    "    url = base_url + my_tag\n",
    "\n",
    "    driver.get(url)\n",
    "    elem = driver.find_element_by_tag_name('body')\n",
    "    link_list = []\n",
    "\n",
    "    pagedowns = 0\n",
    "    while pagedowns < num_of_crawling_pages:\n",
    "\n",
    "        time.sleep(0.5)\n",
    "        links = driver.find_elements_by_css_selector('div.v1Nh3 > a')\n",
    "        for i in links:\n",
    "            link_list.append(i.get_attribute('href'))\n",
    "\n",
    "        for i in range(6):\n",
    "            elem.send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        pagedowns += 1\n",
    "        print(pagedowns, '페이지 크롤링 완료')\n",
    "\n",
    "    set_link_list = list(set(link_list))\n",
    "    num_link_list = len(link_list)\n",
    "    num_set_link_list = len(set_link_list)\n",
    "\n",
    "    print('중복링크 개수', num_link_list)\n",
    "    print('유니크링크 개수', num_set_link_list)\n",
    "    print('유니크링크 / 중복링크 : ', round((num_set_link_list/num_link_list) * 100, 2), '%')\n",
    "    print('유니크링크 구성')\n",
    "    print(set_link_list[:10])\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    save_list_csv(set_link_list, my_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import emoji\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_list(my_list, my_tag):\n",
    "    tag_name = my_tag\n",
    "    file_name = tag_name + '_tag_link.csv'\n",
    "    \n",
    "    with open(file_name) as file:\n",
    "        for line in file:\n",
    "            read_data = line\n",
    "            my_list.append(read_data[:-2])\n",
    "        file.close()\n",
    "    print('load ', len(my_list), 'data from csv to list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data2json(my_tag, id, username, date, contents, hashtags, final_image_link, likes_num, comments_num):\n",
    "    single_data = {\n",
    "        \"find_tag\" : my_tag,\n",
    "        \"id\" : id,\n",
    "        \"username\" : username,\n",
    "        \"date\" : date,\n",
    "        \"contents\" : contents,\n",
    "        \"hashtags\" : hashtags,\n",
    "        \"imagelinks\" : final_image_link,\n",
    "        \"likes\" : likes_num,\n",
    "        \"comments\" : comments_num\n",
    "    }\n",
    "    return single_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jsonlist_json(myjson):\n",
    "    file_name = my_tag + '_rawdata.json'\n",
    "    with open(file_name, \"w\") as file:\n",
    "        for line in myjson:\n",
    "            file.write(str(line) + ',\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json_file(myjson):\n",
    "    file_name = my_tag + '_rawdata.json'\n",
    "    with open(file_name, \"w\") as file:\n",
    "        for line in myjson:\n",
    "            file.write(str(line) + ',\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling_rawdata(my_tag):\n",
    "    my_links = []\n",
    "    read_csv_list(my_links, my_tag)\n",
    "\n",
    "    json_list = []\n",
    "    emoji_keys = emoji.UNICODE_EMOJI.keys()\n",
    "    \n",
    "    for i in range(len(my_links)-1):\n",
    "        url = my_links[i]\n",
    "        req = requests.get(url)\n",
    "        html = req.text\n",
    "        header = req.headers\n",
    "        status = req.status_code\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        script_contents = soup.find_all('script')\n",
    "        \n",
    "        ## id\n",
    "        id_p = re.compile(\"\\/p\\/(.*?)\\/\")\n",
    "        id = id_p.findall(url)[0]\n",
    "        \n",
    "        ## username\n",
    "        user_content = soup.find(rel = \"canonical\")\n",
    "        if not user_content:\n",
    "            username = ''\n",
    "        else:\n",
    "            username_href = user_content.get('href')\n",
    "            username_p = re.compile(\"instagram.com/(.*)/p\")\n",
    "            username = username_p.findall(str(username_href))\n",
    "            if len(username) != 0:\n",
    "                username = username[0]\n",
    "            else:\n",
    "                username = ''\n",
    "\n",
    "        ## date\n",
    "        date_p = re.compile(\"uploadDate\\\":\\\"(.{19})\\\"\")\n",
    "        date = date_p.findall(str(script_contents))\n",
    "        if len(date) != 0:\n",
    "            date = date[0]\n",
    "        else:\n",
    "            date = ''\n",
    "        \n",
    "        ## contents\n",
    "        contents_p = re.compile(\"\\\"caption\\\":\\\"(.*?)\\\"\")\n",
    "        contents = contents_p.findall(str(script_contents))\n",
    "        if len(contents) == 0:\n",
    "            contents_p = re.compile(\"\\\"edge_media_to_caption\\\".*?text\\\":\\\"(.*?)\\\"\")\n",
    "            contents = contents_p.findall(str(script_contents))\n",
    "            if len(contents) == 0:\n",
    "                contents = ['']\n",
    "                \n",
    "        contents = contents[0]\n",
    "        contents = contents.encode('utf-8')\n",
    "        contents = contents.decode('unicode_escape')\n",
    "        contents = contents.encode('utf-8','ignore')\n",
    "        contents = contents.decode('utf-8')\n",
    "        \n",
    "        \n",
    "        ## hashtags\n",
    "        meta_content = soup.find_all(property = \"instapp:hashtags\")\n",
    "        hashtags = []    \n",
    "        if meta_content:\n",
    "            hash_tags_p = re.compile(\"content=\\\"(.*?)\\\"\")\n",
    "            emoji_hashtag = hash_tags_p.findall(str(meta_content))\n",
    "            \n",
    "            for tag in emoji_hashtag:\n",
    "                for e in emoji_keys:\n",
    "                    emoji_have = tag.find(e)\n",
    "                    if emoji_have > -1:\n",
    "                        tag = tag.replace(e, '')\n",
    "                hashtags.append(tag)\n",
    "            hashtags = list(filter(None, hashtags))\n",
    "\n",
    "        ## remove hashtag from contents\n",
    "        for tag in hashtags:\n",
    "            tag = '#' + tag\n",
    "            contents = contents.replace(tag, '', 1)\n",
    "        \n",
    "        ## preprocessing of contents\n",
    "        contents = contents.replace('#', '')\n",
    "        contents = re.sub(\"\\\\\\\\u[0-9A-Fa-f]{4}\", \"\", contents)\n",
    "        contents = re.sub(\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", contents)\n",
    "        contents = re.sub('\\n', '', contents)\n",
    "        contents = re.sub('\\s+', ' ', contents)\n",
    "            \n",
    "\n",
    "        ## image links\n",
    "        image_links_p = re.compile(\"\\\"display_resources\\\":\\[.*?\\]\")\n",
    "        image_links = image_links_p.findall(str(script_contents))\n",
    "        \n",
    "        final_image_link = []\n",
    "        single_image_link_p = re.compile(\"\\\"src\\\":\\\"(.*?)\\\"\")\n",
    "        for dif_img in image_links:\n",
    "            link = single_image_link_p.findall(str(dif_img))[2]\n",
    "            final_image_link.append(link)\n",
    "            \n",
    "        if len(final_image_link) != 1:\n",
    "            final_image_link = final_image_link[1:]   \n",
    "            \n",
    "        \n",
    "        ## likes num\n",
    "        likes_num_p = re.compile(\"\\\"description\\\":\\\"(.*?)Likes\")\n",
    "        likes_num = likes_num_p.findall(str(script_contents))\n",
    "\n",
    "        if len(likes_num) > 0:\n",
    "            likes_num = likes_num[0]\n",
    "            likes_num = re.sub('[,\\s]', '', likes_num)\n",
    "        else:\n",
    "            likes_num = '0'\n",
    "        \n",
    "        ## comments num\n",
    "        comments_num_p = re.compile(\"\\\"description\\\":\\\".*?,(.*?)Comments\")\n",
    "        comments_num = comments_num_p.findall(str(script_contents))\n",
    "\n",
    "        if len(comments_num) > 0:\n",
    "            comments_num = comments_num[0]\n",
    "            comments_num = re.sub('[,\\s]', '', comments_num)\n",
    "        else:\n",
    "            comments_num = '0'\n",
    "        \n",
    "        ## save data as json\n",
    "        single_json = data2json(my_tag, id, username, date, contents, hashtags, final_image_link, likes_num, comments_num)\n",
    "        json_list.append(single_json)\n",
    "        time.sleep(0.01)\n",
    "        if i % 30 == 0:\n",
    "            print(i, '번째 데이터')\n",
    "            print(single_json)\n",
    "            print('------------------')\n",
    "                \n",
    "    save_json_file(json_list)\n",
    "    print('Crawling 완료!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your tag :cat\n",
      "Enter num of crawling pages :1\n",
      "1 페이지 크롤링 완료\n",
      "중복링크 개수 33\n",
      "유니크링크 개수 33\n",
      "유니크링크 / 중복링크 :  100.0 %\n",
      "유니크링크 구성\n",
      "['https://www.instagram.com/p/BshxLFplVd9/', 'https://www.instagram.com/p/BshxLPRhHcf/', 'https://www.instagram.com/p/BshURTEBK3b/', 'https://www.instagram.com/p/BshxI1Gg9nm/', 'https://www.instagram.com/p/BshxJP_AslI/', 'https://www.instagram.com/p/BshxKvKhRJ4/', 'https://www.instagram.com/p/BshxJmej4E_/', 'https://www.instagram.com/p/BshMo2OAc-x/', 'https://www.instagram.com/p/BshxLMTg44Q/', 'https://www.instagram.com/p/BshVEeKgtLV/']\n",
      "load  33 data from csv to list\n",
      "0 번째 데이터\n",
      "{'find_tag': 'cat', 'id': 'BshxLFplVd9', 'username': 'piero.sewing', 'date': '2019-01-12T08:22:12', 'contents': '圓筒揹袋 33L x 22dia cm 揹帶全長100cm歡迎查詢訂購 ', 'hashtags': ['cat', 'tote_bag', '鈎', '手挽袋', '和尚袋', '訂製', '訂購', '織', '萬用袋', 'pac_man', 'alice_in_the_wonderland', '索袋', '斜揹袋', '自家手作', '縫', 'tailor_made', '圓筒袋'], 'imagelinks': ['https://scontent-icn1-1.cdninstagram.com/vp/bab33b0a2e8f7dea7abf56c5e9be748d/5CBC9E26/t51.2885-15/e35/49660308_2103469289696922_3055289021044589620_n.jpg?_nc_ht=scontent-icn1-1.cdninstagram.com'], 'likes': '0', 'comments': '0'}\n",
      "------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:54: DeprecationWarning: invalid escape sequence '\\/'\n"
     ]
    }
   ],
   "source": [
    "my_tag = input('Enter your tag :')\n",
    "num_of_crawling_pages = int(input('Enter num of crawling pages :'))\n",
    "crawling_links(my_tag, num_of_crawling_pages)\n",
    "crawling_rawdata(my_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
